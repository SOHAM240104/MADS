2025-09-25 16:41:00,228 - INFO - Using device: mps
2025-09-25 16:41:00,228 - INFO - ================================================================================
2025-09-25 16:41:00,228 - INFO - SEQUENCE-TO-SEQUENCE MODEL WITH BI-LSTM AND ATTENTION
2025-09-25 16:41:00,228 - INFO - ================================================================================
2025-09-25 16:41:00,228 - INFO - PHASE 1: Training on bash-only dataset
2025-09-25 16:41:00,228 - INFO - ----------------------------------------
2025-09-25 16:41:00,778 - INFO - Model created with 20,872,064 trainable parameters
2025-09-25 16:41:00,778 - INFO - Bash vocabulary sizes - Source: 11013, Target: 1092
2025-09-25 16:41:00,778 - INFO - Starting PHASE_1...
2025-09-25 16:41:06,297 - INFO -   Batch 0/3548, Loss: 6.9958
2025-09-25 16:41:22,915 - INFO -   Batch 100/3548, Loss: 2.9304
2025-09-25 16:41:37,927 - INFO -   Batch 200/3548, Loss: 3.1707
2025-09-25 16:41:52,697 - INFO -   Batch 300/3548, Loss: 3.7318
2025-09-25 16:42:07,527 - INFO -   Batch 400/3548, Loss: 3.3062
2025-09-25 16:42:21,837 - INFO -   Batch 500/3548, Loss: 2.4285
2025-09-25 16:42:37,450 - INFO -   Batch 600/3548, Loss: 2.0960
2025-09-25 16:42:52,041 - INFO -   Batch 700/3548, Loss: 2.1844
2025-09-25 16:43:06,841 - INFO -   Batch 800/3548, Loss: 1.8922
2025-09-25 16:43:21,741 - INFO -   Batch 900/3548, Loss: 2.1328
2025-09-25 16:43:36,474 - INFO -   Batch 1000/3548, Loss: 2.6477
2025-09-25 16:43:50,754 - INFO -   Batch 1100/3548, Loss: 2.8088
